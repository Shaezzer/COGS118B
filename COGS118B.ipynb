{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "import inflect\n",
    "import re\n",
    "from itertools import chain\n",
    "#!pip install contractions\n",
    "import contractions\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nuc24nwLI46K"
   },
   "outputs": [],
   "source": [
    "#Loading and Cleaning Data\n",
    "\n",
    "#load in the csv contatining the data\n",
    "df_original = pd.read_csv('wiki_movie_plots_deduped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpVQi0yQI9fy"
   },
   "outputs": [],
   "source": [
    "#drop all rows in the dataframe that do not contain horror(1167) in genre catergory\n",
    "df_horror = df_original.drop(df_original[df_original['Genre'] != 'horror'].index,inplace=False)\n",
    "#drop columns we wont need\n",
    "df_horror.drop(['Release Year','Origin/Ethnicity','Cast', 'Wiki Page','Director','Release Year'],axis=1, \n",
    "               inplace = True)\n",
    "\n",
    "#reduce horror to 1000 rows\n",
    "df_horror = df_horror.sample(frac=1)\n",
    "df_horror = df_horror.reset_index(drop=True)\n",
    "df_horror_train = df_horror[0:1000]\n",
    "df_horror_test = df_horror[1000:len(df_horror)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAwh4SxaJF0s"
   },
   "outputs": [],
   "source": [
    "#drop all rows in dataframe that do not contain comedy(4379)\n",
    "df_comedy = df_original.drop(df_original[df_original['Genre'] != 'comedy'].index, inplace = False)\n",
    "df_comedy.drop(['Release Year','Origin/Ethnicity','Cast', 'Wiki Page','Director','Release Year'],axis=1, \n",
    "               inplace = True)\n",
    "#reduce horror to 1000 rows\n",
    "df_comedy = df_comedy.sample(frac=1)\n",
    "df_comedy = df_comedy.reset_index(drop=True)\n",
    "df_comedy_train = df_comedy[0:1000]\n",
    "df_comedy_test = df_comedy[1000:len(df_comedy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4-RZfoyJUvr"
   },
   "outputs": [],
   "source": [
    "#merge the comedy and horror dataframes into one\n",
    "df_HandC = pd.concat([df_horror_test,df_comedy_test])\n",
    "\n",
    "#Pre punctuation parsing test\n",
    "#df_HandC.iloc[1002]['Plot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check word type of a word\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for text pre-processing\n",
    "def normalizeText(row):\n",
    "    \n",
    "    #create a list of punctuations we wish to delete\n",
    "    punctuations = string.punctuation\n",
    "    #create a list of stopwords in English\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    #change all letters to lower case\n",
    "    row = row.lower()  \n",
    "    \n",
    "    #remove numbers\n",
    "    row = re.sub(\" \\d+\", \" \", row)\n",
    "                 \n",
    "    #remove punctutation\n",
    "    for letter in row: \n",
    "    \n",
    "        if letter in punctuations: \n",
    "            row = row.replace(letter, \"\")\n",
    "    \n",
    "    #expand the contraction I'm -> I am\n",
    "    row = contractions.fix(row)\n",
    "    # got it from (https://github.com/kootenpv/contractions)\n",
    "                \n",
    "    #remove accent char\n",
    "    row = unicodedata.normalize('NFKD', row).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "                 \n",
    "    #remove extra whitespace convert into a word  \n",
    "    row = row.strip()\n",
    "    \n",
    "    # TOKENIZATION: process of splitting text into smaller piece called tokens.\n",
    "    tokens = word_tokenize(row)\n",
    "    \n",
    "    # lemmatization step played -> play\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    row = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(row)])\n",
    "    \n",
    "    #remove stop words such as \"a\", \"the\", \"is\"\n",
    "    tokens = word_tokenize(row)\n",
    "    row = ' '.join([i for i in tokens if not i in stopWords])\n",
    "     \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the column through the normalizing function\n",
    "df_HandC[\"Plot\"] = df_HandC[\"Plot\"].apply(normalizeText) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "30Jx7UPDIMmN",
    "outputId": "18cdbc95-bf9a-4d30-e946-697c1ccdd068"
   },
   "outputs": [],
   "source": [
    "# Make list from df for \"Plot\" column\n",
    "# Param: the df to use\n",
    "def dfToList(df):\n",
    "    lst = []\n",
    "    for i, rows in df.iterrows():\n",
    "        l = [rows.Plot]\n",
    "        lst.append(l)\n",
    "    return list(chain.from_iterable(lst))\n",
    "\n",
    "# tf-idf vectorisation\n",
    "# Param: all plots from the df as a list (ie [plot1, plot2,...])\n",
    "def tfidfVec(plots):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform(plots)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = tfidf.todense()\n",
    "    df_tfidf = pd.DataFrame(dense.tolist(), columns=feature_names)\n",
    "    return df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abomination</th>\n",
       "      <th>accidentally</th>\n",
       "      <th>accord</th>\n",
       "      <th>action</th>\n",
       "      <th>actually</th>\n",
       "      <th>ad</th>\n",
       "      <th>addiction</th>\n",
       "      <th>adhyayan</th>\n",
       "      <th>admits</th>\n",
       "      <th>...</th>\n",
       "      <th>wood</th>\n",
       "      <th>worry</th>\n",
       "      <th>wound</th>\n",
       "      <th>wrist</th>\n",
       "      <th>write</th>\n",
       "      <th>yash</th>\n",
       "      <th>year</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youngadult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021758</td>\n",
       "      <td>0.043516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021758</td>\n",
       "      <td>0.021758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017554</td>\n",
       "      <td>0.043516</td>\n",
       "      <td>0.070216</td>\n",
       "      <td>0.282851</td>\n",
       "      <td>0.012258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022645</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 877 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    abandon  abomination  accidentally    accord    action  actually  \\\n",
       "0  0.000000     0.000000      0.014571  0.000000  0.021758  0.043516   \n",
       "1  0.000000     0.090579      0.000000  0.022645  0.000000  0.000000   \n",
       "2  0.027365     0.000000      0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.024539     0.000000      0.020370  0.000000  0.000000  0.000000   \n",
       "4  0.000000     0.000000      0.021862  0.000000  0.000000  0.000000   \n",
       "\n",
       "         ad  addiction  adhyayan    admits     ...          wood     worry  \\\n",
       "0  0.000000   0.000000  0.021758  0.021758     ...      0.000000  0.000000   \n",
       "1  0.000000   0.022645  0.000000  0.000000     ...      0.091348  0.000000   \n",
       "2  0.000000   0.000000  0.000000  0.000000     ...      0.000000  0.000000   \n",
       "3  0.030415   0.000000  0.000000  0.000000     ...      0.049078  0.000000   \n",
       "4  0.000000   0.000000  0.000000  0.000000     ...      0.000000  0.032643   \n",
       "\n",
       "      wound     wrist     write      yash      year      york     young  \\\n",
       "0  0.017554  0.043516  0.070216  0.282851  0.012258  0.000000  0.000000   \n",
       "1  0.091348  0.000000  0.018270  0.000000  0.012758  0.000000  0.022645   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.019109  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.017135  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.065287  0.000000   \n",
       "\n",
       "   youngadult  \n",
       "0    0.000000  \n",
       "1    0.000000  \n",
       "2    0.000000  \n",
       "3    0.000000  \n",
       "4    0.032643  \n",
       "\n",
       "[5 rows x 877 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test showing how it works on first 5 elements of df_HandC\n",
    "tfidfVec(dfToList(df_HandC.head()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COGS118B.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
