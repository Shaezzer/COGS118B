{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COGS118B.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEsbVd9tkqMY",
        "colab_type": "code",
        "outputId": "e809fbd6-12a5-4ffd-9b7a-6c8e77b639bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "#!pip install nltk\n",
        "import nltk\n",
        "#nltk.download()\n",
        "#!pip install sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nuc24nwLI46K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading and Cleaning Data\n",
        "\n",
        "#load in the csv contatining the data\n",
        "df_original = pd.read_csv('wiki_movie_plots_deduped.csv.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpVQi0yQI9fy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drop all rows in the dataframe that do not contain horror(1167) in genre catergory\n",
        "df_horror=df_original.drop(df_original[df_original['Genre'] != 'horror'].index,inplace=False)\n",
        "#drop columns we wont need\n",
        "df_horror.drop(['Release Year','Origin/Ethnicity','Cast', 'Wiki Page','Director','Release Year'],axis=1, inplace = True)\n",
        "\n",
        "#reduce horror to 1000 rows\n",
        "df_horror=df_horror.sample(frac=1)\n",
        "df_horror=df_horror.reset_index(drop=True)\n",
        "df_horror_train=df_horror[0:1000]\n",
        "df_horror_test=df_horror[1000:1167]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAwh4SxaJF0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drop all rows in dataframe that do not contain comedy(4379)\n",
        "df_comedy=df_original.drop(df_original[df_original['Genre'] != 'comedy'].index, inplace = False)\n",
        "df_comedy.drop(['Release Year','Origin/Ethnicity','Cast', 'Wiki Page','Director','Release Year'],axis=1, inplace = True)\n",
        "#reduce horror to 1000 rows\n",
        "df_comedy=df_comedy.sample(frac=1)\n",
        "df_comedy=df_comedy.reset_index(drop=True)\n",
        "df_comedy_train=df_comedy[0:1000]\n",
        "df_comedy_test=df_comedy[1000:len(df_comedy)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4-RZfoyJUvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#merge the comedy and horror dataframes into one\n",
        "df_HandC=pd.concat([df_horror_test,df_comedy_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shQM5G5sMjJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalizeText(plotSummary):\n",
        "  normalized_plotSummary = []\n",
        "    \n",
        "  for word in plotSummary:\n",
        "\t\t# remove accented characters\n",
        "    word= remove_accented_chars(word)\n",
        "\t\t# expand contractions  (I'm -> I am)  \n",
        "    word = expand_contractions(word)\n",
        "        # lowercase the text  (We -> we)  \n",
        "    word = word.lower()\n",
        "        # lemmatize text (ran/running is same as run)\n",
        "    word = nltk.lemmatize_text(word)\n",
        "\n",
        "         # remove special characters and\\or digits   \n",
        "         # insert spaces between special characters to isolate them    \n",
        "    special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "    word = special_char_pattern.sub(\" \\\\1 \", word)\n",
        "        # remove digits\n",
        "    word = nltk.remove_special_characters(word, remove_digits=remove_digits)  \n",
        "        \n",
        "        # remove extra whitespace\n",
        "    word = re.sub(' +', ' ', word)\n",
        "        # remove stopwords (words like the/a/for have no sentiment)\n",
        "    word = nltk.remove_stopwords(word, is_lower_case=text_lower_case)\n",
        "\n",
        "    normalized_plotSummary.append(word)\n",
        "\n",
        "  return normalized_plotSummary\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLdPVNgzN1Bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run normalizeText on df_HandC\n",
        "#for row in df_HandC['Plot']:\n",
        "#  row = normalizeText(row)\n",
        "  #print(row)\n",
        "#df_HandC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Jx7UPDIMmN",
        "colab_type": "code",
        "outputId": "18cdbc95-bf9a-4d30-e946-697c1ccdd068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "##Prabesh being stupid. might be helpful?\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf = vectorizer.fit_transform(df_HandC['Plot'])\n",
        "#print similarity matrix\n",
        "print((tfidf*tfidf.T).A)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.09621556 0.09798233 ... 0.10929161 0.06588239 0.06927695]\n",
            " [0.09621556 1.         0.14601356 ... 0.11971793 0.08860325 0.09084843]\n",
            " [0.09798233 0.14601356 1.         ... 0.14489609 0.10433295 0.10066875]\n",
            " ...\n",
            " [0.10929161 0.11971793 0.14489609 ... 1.         0.09237811 0.08509669]\n",
            " [0.06588239 0.08860325 0.10433295 ... 0.09237811 1.         0.07274998]\n",
            " [0.06927695 0.09084843 0.10066875 ... 0.08509669 0.07274998 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi03OFl7PJLW",
        "colab_type": "code",
        "outputId": "f4ef8a43-3d52-4197-969c-4dbf456cbca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "##Prabesh being stupid. might be helpful?\n",
        "\n",
        "#deleting the punctuation\n",
        "without_punct = [w for w in df_HandC['Plot'] if w not in punctuation]\n",
        "#deleting stopwords like the/we/for\n",
        "sw = stopwords.words('english')\n",
        "without_sw = [w for w in without_punct if w not in sw]\n",
        "#lemmatizing the plot summary\n",
        "wnl = WordNetLemmatizer()\n",
        "lemmatizedSummary = wnl.lemmatize(without_sw)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-9118c73b1a3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#lemmatizing the plot summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwnl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlemmatizedSummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwithout_sw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   1794\u001b[0m         \u001b[0;31m# 0. Check the exception lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_exceptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1796\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1797\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    }
  ]
}