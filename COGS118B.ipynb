{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "evalue": "Error: Activating Python 3.7.3 64-bit to run Jupyter failed with Error: Command failed: C:/Users/sosfr/Anaconda3/Scripts/activate && conda activate base && echo 'e8b39361-0157-4923-80e1-22d70d46dee6' && python c:/Users/sosfr/.vscode/extensions/ms-python.python-2019.11.50794/pythonFiles/printEnvVariables.py\nThe process cannot access the file because it is being used by another process.\r\nThe process cannot access the file because it is being used by another process.\r\nThe system cannot find the file C:\\Users\\sosfr\\AppData\\Local\\Temp\\conda-25500-3929.tmp.\r\nThe process cannot access the file because it is being used by another process.\r\n'conda' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n.",
     "output_type": "error"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "import string\n",
    "#nltk.download()\n",
    "#!pip install sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "#nltk.download('wordnet')\n",
    "import unicodedata\n",
    "#!pip install inflect\n",
    "import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nuc24nwLI46K"
   },
   "outputs": [],
   "source": [
    "#Loading and Cleaning Data\n",
    "\n",
    "#load in the csv contatining the data\n",
    "df_original = pd.read_csv('wiki_movie_plots_deduped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpVQi0yQI9fy"
   },
   "outputs": [],
   "source": [
    "#drop all rows in the dataframe that do not contain horror(1167) in genre catergory\n",
    "df_horror = df_original.drop(df_original[df_original['Genre'] != 'horror'].index,inplace=False)\n",
    "#drop columns we wont need\n",
    "df_horror.drop(['Release Year','Origin/Ethnicity','Cast', 'Wiki Page','Director','Release Year'],axis=1, \n",
    "               inplace = True)\n",
    "\n",
    "#reduce horror to 1000 rows\n",
    "df_horror = df_horror.sample(frac=1)\n",
    "df_horror = df_horror.reset_index(drop=True)\n",
    "df_horror_train = df_horror[0:1000]\n",
    "df_horror_test = df_horror[1000:len(df_horror)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAwh4SxaJF0s"
   },
   "outputs": [],
   "source": [
    "#drop all rows in dataframe that do not contain comedy(4379)\n",
    "df_comedy = df_original.drop(df_original[df_original['Genre'] != 'comedy'].index, inplace = False)\n",
    "df_comedy.drop(['Release Year','Origin/Ethnicity','Cast', 'Wiki Page','Director','Release Year'],axis=1, \n",
    "               inplace = True)\n",
    "#reduce horror to 1000 rows\n",
    "df_comedy = df_comedy.sample(frac=1)\n",
    "df_comedy = df_comedy.reset_index(drop=True)\n",
    "df_comedy_train = df_comedy[0:1000]\n",
    "df_comedy_test = df_comedy[1000:len(df_comedy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4-RZfoyJUvr"
   },
   "outputs": [],
   "source": [
    "#merge the comedy and horror dataframes into one\n",
    "df_HandC = pd.concat([df_horror_test,df_comedy_test])\n",
    "\n",
    "#Pre punctuation parsing test\n",
    "#df_HandC.iloc[1002]['Plot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_HandC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2f62bd9f3f69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#run the column through the normalizing function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mdf_HandC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Plot\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_HandC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Plot\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalizeText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_HandC' is not defined"
     ]
    }
   ],
   "source": [
    "#Function to get rid of punctuation\n",
    "def normalizeText(row):\n",
    "    #create a list of punctuations we wish to delete\n",
    "    punctuations = string.punctuation\n",
    "    #change all letters to lower case\n",
    "    row=row.lower()\n",
    "    #loop through each word and get rid of punctuation and \\r\\n that came with data\n",
    "    for x in row: \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatizer.lemmatize(x)\n",
    "        if x in punctuations: \n",
    "            row = row.replace(x, \"\")\n",
    "        row = row.replace('\\r\\n', '')\n",
    "    return row\n",
    "#run the column through the normalizing function\n",
    "df_HandC[\"Plot\"] = df_HandC[\"Plot\"].apply(normalizeText) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove accented characters.\n",
    "# Param: entire string will be converted, punctuation accepted\n",
    "def removeAccents(string):\n",
    "    return unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace numbers with word representation.\n",
    "# NOTE: run before removing punctuation, result will have punctuation\n",
    "# Param: single num, no punctuation or spaces within the string\n",
    "def numToWord(num):\n",
    "    try:\n",
    "        float(num)\n",
    "        return inflect.engine().number_to_words(num)\n",
    "    except ValueError:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iLdPVNgzN1Bx"
   },
   "outputs": [],
   "source": [
    "#Post Punctuation Parsing test\n",
    "#df_HandC.iloc[1002]['Plot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "30Jx7UPDIMmN",
    "outputId": "18cdbc95-bf9a-4d30-e946-697c1ccdd068"
   },
   "outputs": [],
   "source": [
    "##Prabesh being stupid. might be helpful?\n",
    "\n",
    "#vectorizer = TfidfVectorizer()\n",
    "#tfidf = vectorizer.fit_transform(df_HandC['Plot'])\n",
    "#print similarity matrix\n",
    "#print((tfidf*tfidf.T).A)\n",
    "\n",
    "# Make list from df for \"Plot\" column\n",
    "# Param: the df to use\n",
    "from itertools import chain\n",
    "def dfToList(df):\n",
    "    lst = []\n",
    "    for i, rows in df.iterrows():\n",
    "        l = [rows.Plot]\n",
    "        lst.append(l)\n",
    "    return list(chain.from_iterable(lst))\n",
    "\n",
    "# tf-idf vectorisation\n",
    "# Param: all plots from the df as a list (ie [plot1, plot2,...])\n",
    "def tfidfVec(plots):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform(plots)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = tfidf.todense()\n",
    "    df_tfidf = pd.DataFrame(dense.tolist(), columns=feature_names)\n",
    "    return df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>3s</th>\n",
       "      <th>about</th>\n",
       "      <th>abused</th>\n",
       "      <th>accepts</th>\n",
       "      <th>accidentally</th>\n",
       "      <th>act</th>\n",
       "      <th>actor</th>\n",
       "      <th>actress</th>\n",
       "      <th>admits</th>\n",
       "      <th>...</th>\n",
       "      <th>woman</th>\n",
       "      <th>womens</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>worry</th>\n",
       "      <th>would</th>\n",
       "      <th>wound</th>\n",
       "      <th>years</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018012</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>0.029065</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014532</td>\n",
       "      <td>0.014532</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.08184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021381</td>\n",
       "      <td>0.021381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021381</td>\n",
       "      <td>0.017250</td>\n",
       "      <td>0.017250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021381</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 671 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        100        3s     about    abused   accepts  accidentally       act  \\\n",
       "0  0.018012  0.018012  0.000000  0.018012  0.018012      0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.066028  0.000000  0.000000      0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.051749  0.000000  0.000000      0.021381  0.021381   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  0.000000   \n",
       "\n",
       "      actor   actress    admits   ...        woman    womens      work  \\\n",
       "0  0.018012  0.029065  0.018012   ...     0.000000  0.018012  0.000000   \n",
       "1  0.000000  0.000000  0.000000   ...     0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.066028  0.000000   ...     0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000   ...     0.042761  0.000000  0.021381   \n",
       "4  0.000000  0.000000  0.000000   ...     0.000000  0.000000  0.000000   \n",
       "\n",
       "    working     worry     would     wound     years      york    young  \n",
       "0  0.018012  0.000000  0.014532  0.014532  0.018012  0.000000  0.00000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.08184  \n",
       "3  0.000000  0.021381  0.017250  0.017250  0.000000  0.021381  0.00000  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  \n",
       "\n",
       "[5 rows x 671 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test showing how it works on first 5 elements of df_HandC\n",
    "tfidfVec(dfToList(df_HandC.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "mi03OFl7PJLW",
    "outputId": "f4ef8a43-3d52-4197-969c-4dbf456cbca6"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-7b52f1ac2219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#lemmatizing the plot summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mwnl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlemmatizedSummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwithout_sw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   1906\u001b[0m         \u001b[0;31m# 0. Check the exception lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1907\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_exceptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1908\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1909\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "##Prabesh being stupid. might be helpful?\n",
    "\n",
    "#deleting the punctuation\n",
    "without_punct = [w for w in df_HandC['Plot'] if w not in punctuation]\n",
    "#deleting stopwords like the/we/for\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "without_sw = [w for w in without_punct if w not in sw]\n",
    "#lemmatizing the plot summary\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmatizedSummary = wnl.lemmatize(without_sw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COGS118B.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}